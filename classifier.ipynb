{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn import model_selection\n",
    "\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: I added these to the gitignore\n",
    "model_save_files = ['stored_models.pickle']\n",
    "info_save_files = ['model_info.pickle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_csv('archive/credit_card_transactions.csv')\n",
    "\n",
    "#create a copy in which the fraud flag is enabled, for later reference\n",
    "df_transactions_copy = df_transactions.copy()\n",
    "\n",
    "#these rows are all useless/would cause bad outcomes\n",
    "# df_transactions = df_transactions.drop(columns=['is_fraud', 'first', 'last', 'city', 'street', 'state', 'trans_num', 'trans_date_trans_time', 'cc_num', 'merchant', 'lat', 'long', 'city_pop', 'job', 'dob', 'merch_long', 'merch_zipcode'])\n",
    "df_transactions = df_transactions.drop(columns=['Unnamed: 0', 'is_fraud', 'first', 'last', 'city', 'street', 'state', 'zip', 'trans_num', 'unix_time', 'trans_date_trans_time', 'cc_num', 'merchant', 'lat', 'long', 'city_pop', 'job', 'dob', 'merch_lat', 'merch_long', 'merch_zipcode'])\n",
    "\n",
    "transactions = df_transactions.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_transactions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_ids = df_transactions.columns\n",
    "\n",
    "unique_indices = {}\n",
    "unique_indices_values = {}\n",
    "\n",
    "X = np.ndarray(transactions.shape)\n",
    "\n",
    "zero_time = datetime.fromordinal(1)\n",
    "\n",
    "#this section is kind of abstract\n",
    "#for every element of the dataset\n",
    "for i in range(X.shape[0]):\n",
    "    #for every component of that element\n",
    "    for j in range(X.shape[1]):\n",
    "        #if that component is a number, it stays as it is\n",
    "        if isinstance(transactions[i, j], int) or isinstance(transactions[i, j], float):\n",
    "            X[i, j] = transactions[i, j]\n",
    "        \n",
    "        #handle datetime as seconds since 0\n",
    "        elif column_ids[j] == 'trans_date_trans_time':\n",
    "            #convert string to datetime object\n",
    "            dt = datetime.strptime(transactions[i, j], \"%Y-%m-%d %H:%M:%S\")\n",
    "            #convert to timedelta object by subtracting the 0 time\n",
    "            td = dt - zero_time\n",
    "\n",
    "            X[i, j] = td.total_seconds()\n",
    "\n",
    "        #handle dob as seconds since 0\n",
    "        elif column_ids[j] == 'dob':\n",
    "            #convert string to datetime object\n",
    "            dt = datetime.strptime(transactions[i, j], \"%Y-%m-%d\")\n",
    "            #convert to timedelta object by subtracting the 0 time\n",
    "            td = dt - zero_time\n",
    "\n",
    "            X[i, j] = td.total_seconds()\n",
    "\n",
    "        #if it is not a number, assign an integer to each unique value which appears\n",
    "        #ex, if name was a column, we might have 'Bob'=1, 'Cindy'=2, etc\n",
    "        else:\n",
    "            if not j in unique_indices:\n",
    "                unique_indices[j] = {}\n",
    "                unique_indices_values[j] = 0\n",
    "            if not transactions[i, j] in unique_indices[j]:\n",
    "                unique_indices[j][transactions[i, j]] = unique_indices_values[j]\n",
    "                unique_indices_values[j] += 1\n",
    "            X[i, j] = unique_indices[j][transactions[i, j]]\n",
    "        \n",
    "        #replace nan values with 0, since nan is not permissable in a BGM\n",
    "        #might be wise to replace with something else?\n",
    "        if math.isnan(X[i, j]):\n",
    "            X[i, j] = 0\n",
    "\n",
    "#separate the final test set\n",
    "final_test_size = 10000\n",
    "final_test_indices = [i for i in range(X.shape[0] - final_test_size, X.shape[0])]\n",
    "X_test_final = X[final_test_indices]\n",
    "X = X[:-final_test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kept separate so that I don't accidentally fuck up my experiments by running it\n",
    "#use to reset the training\n",
    "trained_models = {}\n",
    "output_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use try catch to prevent errors if the save files are not present\n",
    "\n",
    "for save_file in model_save_files:\n",
    "    try:\n",
    "        #if it exists, load the previously trained models\n",
    "        input_file = open(save_file, 'rb')\n",
    "\n",
    "        #store them back in the trained models dictionary\n",
    "        unpickler = pickle.Unpickler(input_file)\n",
    "        temp_trained_models = unpickler.load()\n",
    "\n",
    "        for model in temp_trained_models:\n",
    "            trained_models[model] = temp_trained_models[model]\n",
    "\n",
    "        input_file.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for save_file in info_save_files:\n",
    "    try:\n",
    "        #if it exists, load the previously trained models\n",
    "        input_file = open(save_file, 'rb')\n",
    "\n",
    "        #store them back in the trained models dictionary\n",
    "        unpickler = pickle.Unpickler(input_file)\n",
    "        temp_output_info = unpickler.load()\n",
    "\n",
    "        for model in temp_trained_models:\n",
    "            output_info[model] = temp_output_info[model]\n",
    "\n",
    "        input_file.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = BayesianGaussianMixture().get_params(0)\n",
    "\n",
    "default['n_components'] = 10 #the defualt is 1, which is useless\n",
    "default['weight_concentration_prior'] = 10 #the default is 0, which is bad\n",
    "default['verbose'] = False #switch to true, to fill your screen with numbers\n",
    "default['max_iter'] = 1000 #defaults to 100, but that often fails to converge\n",
    "default['random_state'] = 0 #to keep experiments consistent\n",
    "\n",
    "def bgm(parameters):\n",
    "    '''Returns a BayesianGaussianMixture based on the parameters\\n\n",
    "    parameters can be either a key or a dictionary\n",
    "    '''\n",
    "    params = dict(parameters)\n",
    "    comps = default\n",
    "\n",
    "    for key in params:\n",
    "        comps[key] = params[key]\n",
    "\n",
    "    return BayesianGaussianMixture(covariance_prior=comps['covariance_prior'],\n",
    "        covariance_type=comps['covariance_type'],\n",
    "        degrees_of_freedom_prior=comps['degrees_of_freedom_prior'],\n",
    "        init_params=comps['init_params'],\n",
    "        max_iter=comps['max_iter'],\n",
    "        mean_precision_prior=comps['mean_precision_prior'],\n",
    "        mean_prior=comps['mean_prior'],\n",
    "        n_components=comps['n_components'],\n",
    "        n_init=comps['n_init'],\n",
    "        random_state=comps['random_state'],\n",
    "        reg_covar=comps['reg_covar'],\n",
    "        tol=comps['tol'],\n",
    "        verbose=comps['verbose'],\n",
    "        verbose_interval=comps['verbose_interval'],\n",
    "        warm_start=comps['warm_start'],\n",
    "        weight_concentration_prior=comps['weight_concentration_prior'],\n",
    "        weight_concentration_prior_type=comps['weight_concentration_prior_type'])\n",
    "\n",
    "def key(dic, additional_params:dict={}):\n",
    "    '''uses a parameter dictionary to save keys for easier lookup, from a dictionary\\n\n",
    "    Ex. trained_models[key({'tol':1.0, 'fold':2})] will give back the model with those parameters'''\n",
    "    dictionary = dict(dic)\n",
    "    for param in additional_params:\n",
    "        dictionary[param] = additional_params[param]\n",
    "\n",
    "    return frozenset(sorted(zip(dictionary.keys(), dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of model weights\n",
    "model_weights = {}\n",
    "\n",
    "#lower bound on true positive values to be accepted (as a percentage)\n",
    "tp_threshold = 0 #0.75\n",
    "\n",
    "#upper bound on false positive values to be accepted (as a percentage)\n",
    "fp_threshold = 1.0 #0.06\n",
    "\n",
    "#for each key to the trained models dictionary\n",
    "for model in trained_models:\n",
    "    #get the model's stats\n",
    "    fp, tp, fn, tn, epsilon = output_info[model]['scores']\n",
    "\n",
    "    #calculate the percentage of true positives\n",
    "    tp_percent = tp/(tp + fn)\n",
    "\n",
    "    #calculate the percentage of false positives\n",
    "    fp_percent = (fp + 1)/(output_info[model]['log_probs'].shape[0]-(tp + fn))\n",
    "\n",
    "    #if the percentages are within the threshold values\n",
    "    if tp_percent >= tp_threshold and fp_percent <= fp_threshold:\n",
    "        #make an entry in the model weights dictionary using the model weight function TODO: Pick good function\n",
    "        model_weights[model] = tp_percent - fp_percent\n",
    "    #if it is not within the threshold\n",
    "    else:\n",
    "        #this is largely redundant, but included to avoid any possible errors\n",
    "        model_weights[model] = 0\n",
    "\n",
    "#display the weights\n",
    "print(f'Model Weights')\n",
    "for model in model_weights:\n",
    "    print(model, model_weights[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data):\n",
    "    '''\n",
    "    classifies the data, by taking the weighted average among the models\n",
    "    returns a normalized float value representing the average conjecture\n",
    "    '''\n",
    "    #construct a np array representing the keys for the model weights, to allow numerical indexing\n",
    "    models = np.array(list(model_weights.keys()))\n",
    "\n",
    "    #calculate the sum of all weights, dividing by this allows for arbitrary weighting functions\n",
    "    normalization_factor = sum(model_weights.values())\n",
    "\n",
    "    #create the classifications array (holds all classifications by all models)\n",
    "    classifications = np.ndarray((models.shape[0], data.shape[0]))\n",
    "\n",
    "    #for each model\n",
    "    for i in range(models.shape[0]):\n",
    "        #get the model\n",
    "        trained_model = trained_models[models[i]]\n",
    "\n",
    "        #calculate the log likelyhoods\n",
    "        log_likelyhoods = trained_model.score_samples(data) \n",
    "\n",
    "        #assign the log likelyhoods above the model's epsilon value to be ligitimate and apply weight\n",
    "        classifications[i, log_likelyhoods >= output_info[models[i]]['scores'][4]] = 0 * model_weights[models[i]]\n",
    "\n",
    "        #assign the log likelyhoods below the model's epsilon value to be fraudulent and apply weight\n",
    "        classifications[i, log_likelyhoods < output_info[models[i]]['scores'][4]] = 1 * model_weights[models[i]]\n",
    "    \n",
    "    #take the sum of the classifications for each data point, normalize\n",
    "    final_classification = np.sum(classifications, axis=0)/normalization_factor\n",
    "    return final_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty array to hold the fraudulence values for the final test set\n",
    "flag_indices = np.ndarray((X_test_final.shape[0]), dtype=int)\n",
    "\n",
    "#for each position in the final test set\n",
    "for i in range(flag_indices.shape[0]):\n",
    "    #assign it the value of the appropriate entry\n",
    "    flag_indices[i] = df_transactions_copy.iloc[final_test_indices[i]][\"is_fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty array to hold the indices from the test set which are classified as fraud\n",
    "fraud_result_indices = []\n",
    "\n",
    "#the cutoff point for being marked as fraud (lower values allow more fraud, higer values require more strict agreement)\n",
    "fraud_cutoff = 0.5\n",
    "\n",
    "#get the classifications for the final test set\n",
    "clas = classify(X_test_final)\n",
    "\n",
    "#for each classification\n",
    "for i in range(clas.shape[0]):\n",
    "    #if the classification is past the cutoff\n",
    "    if clas[i] != 0 and clas[i] >= fraud_cutoff:\n",
    "        #add the index to the result indices array\n",
    "        fraud_result_indices.append(i)\n",
    "\n",
    "#initialize the false positive and true positive values\n",
    "fp = 0\n",
    "tp = 0\n",
    "\n",
    "#for each index marked as fraud\n",
    "for i in range(len(fraud_result_indices)):\n",
    "    #if that index is actually fraud\n",
    "    if flag_indices[fraud_result_indices[i]] == 1:\n",
    "        #increment true positives\n",
    "        tp += 1\n",
    "    #else increment false positives\n",
    "    else: fp += 1\n",
    "\n",
    "#print out the true positive and false positive data\n",
    "print(f'tp: {tp}/132 ({tp/132 * 100}%), fp: {fp}/9868 ({fp/9868 * 100}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the stats, for the benefit of everyone who is not a computer\n",
    "for model in model_weights:\n",
    "\n",
    "    fp, tp, fn, tn, epsilon = output_info[model]['scores']\n",
    "    #number of transactions marked as suspicious\n",
    "    pos = fp + tp\n",
    "    #number of transactions ignored\n",
    "    neg = fn + tn\n",
    "    #number of fraudulent transactions\n",
    "    flags = tp + fn\n",
    "\n",
    "    #human readible output\n",
    "    print(dict(model), f'weight: {model_weights[model]}')\n",
    "    print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "    print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%)')\n",
    "    print(f'incorrectly classified normal: {fp}/{(output_info[model]['log_probs'].shape[0]-flags)} ({fp/(output_info[model]['log_probs'].shape[0]-flags)*100}%)')\n",
    "\n",
    "    #check to make sure the output was generated with the final test set included\n",
    "    if 'test_scores' in output_info[model]:\n",
    "        fp, tp, fn, tn, epsilon = output_info[model]['test_scores']\n",
    "        #number of transactions marked as suspicious\n",
    "        pos = fp + tp\n",
    "        #number of transactions ignored\n",
    "        neg = fn + tn\n",
    "        #number of fraudulent transactions\n",
    "        flags = tp + fn\n",
    "\n",
    "        #human readible output\n",
    "        print(f'*Test Set*')\n",
    "        print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "        print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%)')\n",
    "        print(f'incorrectly classified normal: {fp}/{(output_info[model]['test_log_probs'].shape[0]-flags)} ({fp/(output_info[model]['test_log_probs'].shape[0]-flags)*100}%)')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
