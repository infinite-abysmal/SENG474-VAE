{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn import model_selection\n",
    "\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to either credit or ether; NOTE: credit not currently working for a few later functions\n",
    "dataset = 'credit'\n",
    "\n",
    "#NOTE: I added these to the gitignore\n",
    "model_save_file = 'stored_models.pickle'\n",
    "info_save_file = 'model_info.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'ether':\n",
    "    df_transactions = pd.read_csv('archive/transaction_dataset.csv')\n",
    "\n",
    "    #create a copy in which the fraud flag is enabled, for later reference\n",
    "    df_transactions_copy = df_transactions.copy()\n",
    "\n",
    "    #these rows are all useless/would cause bad outcomes\n",
    "    df_transactions = df_transactions.drop(columns=['FLAG', 'Index'])\n",
    "\n",
    "    transactions = df_transactions.to_numpy()\n",
    "\n",
    "elif dataset == 'credit':\n",
    "    df_transactions = pd.read_csv('archive/credit_card_transactions.csv')\n",
    "\n",
    "    #create a copy in which the fraud flag is enabled, for later reference\n",
    "    df_transactions_copy = df_transactions.copy()\n",
    "\n",
    "    #these rows are all useless/would cause bad outcomes\n",
    "    # df_transactions = df_transactions.drop(columns=['is_fraud', 'first', 'last', 'city', 'street', 'state', 'trans_num', 'trans_date_trans_time', 'cc_num', 'merchant', 'lat', 'long', 'city_pop', 'job', 'dob', 'merch_long', 'merch_zipcode'])\n",
    "    df_transactions = df_transactions.drop(columns=['Unnamed: 0', 'is_fraud', 'first', 'last', 'city', 'street', 'state', 'zip', 'trans_num', 'unix_time', 'trans_date_trans_time', 'cc_num', 'merchant', 'lat', 'long', 'city_pop', 'job', 'dob', 'merch_lat', 'merch_long', 'merch_zipcode'])\n",
    "\n",
    "    transactions = df_transactions.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'credit':\n",
    "    column_ids = df_transactions.columns\n",
    "\n",
    "    unique_indices = {}\n",
    "    unique_indices_values = {}\n",
    "\n",
    "    X = np.ndarray(transactions.shape)\n",
    "\n",
    "    zero_time = datetime.fromordinal(1)\n",
    "\n",
    "    #this section is kind of abstract\n",
    "    #for every element of the dataset\n",
    "    for i in range(X.shape[0]):\n",
    "        #for every component of that element\n",
    "        for j in range(X.shape[1]):\n",
    "            #if that component is a number, it stays as it is\n",
    "            if isinstance(transactions[i, j], int) or isinstance(transactions[i, j], float):\n",
    "                X[i, j] = transactions[i, j]\n",
    "            \n",
    "            #handle datetime as seconds since 0\n",
    "            elif column_ids[j] == 'trans_date_trans_time':\n",
    "                #convert string to datetime object\n",
    "                dt = datetime.strptime(transactions[i, j], \"%Y-%m-%d %H:%M:%S\")\n",
    "                #convert to timedelta object by subtracting the 0 time\n",
    "                td = dt - zero_time\n",
    "\n",
    "                X[i, j] = td.total_seconds()\n",
    "\n",
    "            #handle dob as seconds since 0\n",
    "            elif column_ids[j] == 'dob':\n",
    "                #convert string to datetime object\n",
    "                dt = datetime.strptime(transactions[i, j], \"%Y-%m-%d\")\n",
    "                #convert to timedelta object by subtracting the 0 time\n",
    "                td = dt - zero_time\n",
    "\n",
    "                X[i, j] = td.total_seconds()\n",
    "\n",
    "            #if it is not a number, assign an integer to each unique value which appears\n",
    "            #ex, if name was a column, we might have 'Bob'=1, 'Cindy'=2, etc\n",
    "            else:\n",
    "                if not j in unique_indices:\n",
    "                    unique_indices[j] = {}\n",
    "                    unique_indices_values[j] = 0\n",
    "                if not transactions[i, j] in unique_indices[j]:\n",
    "                    unique_indices[j][transactions[i, j]] = unique_indices_values[j]\n",
    "                    unique_indices_values[j] += 1\n",
    "                X[i, j] = unique_indices[j][transactions[i, j]]\n",
    "            \n",
    "            #replace nan values with 0, since nan is not permissable in a BGM\n",
    "            #might be wise to replace with something else?\n",
    "            if math.isnan(X[i, j]):\n",
    "                X[i, j] = 0\n",
    "            \n",
    "    #in the ether dataset, index 13 breaks the solver, for some reason; remove it\n",
    "    # excluded_indices = [13]\n",
    "    # included_indices = [i for i in range(X.shape[1]) if i not in excluded_indices]\n",
    "\n",
    "    #I was pleasantly surprised numpy allowed this type of indexing\n",
    "    # X = X[:, included_indices]\n",
    "\n",
    "\n",
    "    final_test_size = 10000\n",
    "    final_test_indices = [i for i in range(X.shape[0] - final_test_size, X.shape[0])]\n",
    "    X_test_final = X[final_test_indices]\n",
    "    X = X[:-final_test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'ether':\n",
    "\n",
    "    unique_indices = {}\n",
    "    unique_indices_values = {}\n",
    "\n",
    "    X = np.ndarray(transactions.shape)\n",
    "\n",
    "    #this section is kind of abstract\n",
    "    #for every element of the dataset\n",
    "    for i in range(X.shape[0]):\n",
    "        #for every component of that element\n",
    "        for j in range(X.shape[1]):\n",
    "            #if that component is a number, it stays as it is\n",
    "            if isinstance(transactions[i, j], int) or isinstance(transactions[i, j], float):\n",
    "                X[i, j] = transactions[i, j]\n",
    "            \n",
    "            #if it is not a number, assign an integer to each unique value which appears\n",
    "            #ex, if name was a column, we might have 'Bob'=1, 'Cindy'=2, etc\n",
    "            else:\n",
    "                if not j in unique_indices:\n",
    "                    unique_indices[j] = {}\n",
    "                    unique_indices_values[j] = 0\n",
    "                if not transactions[i, j] in unique_indices[j]:\n",
    "                    unique_indices[j][transactions[i, j]] = unique_indices_values[j]\n",
    "                    unique_indices_values[j] += 1\n",
    "                X[i, j] = unique_indices[j][transactions[i, j]]\n",
    "            \n",
    "            #replace nan values with 0, since nan is not permissable in a BGM\n",
    "            #might be wise to replace with something else?\n",
    "            if math.isnan(X[i, j]):\n",
    "                X[i, j] = 0\n",
    "    \n",
    "    #in the ether dataset, index 22 breaks the solver, for some reason; remove it\n",
    "    excluded_indices = [22]\n",
    "    included_indices = [i for i in range(X.shape[1]) if i not in excluded_indices]\n",
    "\n",
    "    #I was pleasantly surprised numpy allowed this type of indexing\n",
    "    X = X[:, included_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements(log_probs:np.ndarray, transactions_df:pd.DataFrame, flag_indices:list, train_size:int=0, \n",
    "                     dynamic_epsilon:bool=True, \n",
    "                     score_function:None=lambda tps,fps,tns,fns: tps*1.3 - fps + tns - fns*1.3, \n",
    "                     epsilon:float=-200, test_range:int=400, test_step:int=5, inverted:bool = False):\n",
    "    '''\n",
    "    gets the number of False Positives, True Positives, False Negatives, \n",
    "    True Negatives, and the best separating epsilon (if dynamic_epsilon=True)\n",
    "    score_function is a maximization function\n",
    "    '''\n",
    "\n",
    "    # determine which epsilon gives the best results (if dynamic is on)\n",
    "    best_score = 0\n",
    "    if dynamic_epsilon:\n",
    "        # for each test epsilon in range \n",
    "        # Max epsilon -> Max epsilon - test range (to prevent cases with insane length)\n",
    "        for test_epsilon in range(int(max(log_probs)), int(max(log_probs))-test_range, -test_step):\n",
    "            tps = 0\n",
    "            fps = 0\n",
    "            tns = 0\n",
    "            fns = 0\n",
    "\n",
    "            # for each probability\n",
    "            for i in range(log_probs.shape[0]):\n",
    "                #get whether or not a data point is fraud; NOTE: needs a rework\n",
    "                if dataset == 'ether': \n",
    "                    flag = transactions_df.iloc[i + train_size][\"FLAG\"]\n",
    "                elif dataset == 'credit': \n",
    "                    flag = transactions_df.iloc[i + train_size]['is_fraud']\n",
    "\n",
    "                if not flag_indices is None:\n",
    "                    if dataset == 'ether': \n",
    "                        flag = transactions_df.iloc[flag_indices[i]][\"FLAG\"]\n",
    "                    elif dataset == 'credit': \n",
    "                        flag = transactions_df.iloc[flag_indices[i]]['is_fraud']\n",
    "                \n",
    "                # if the element is marked as fraud, determine whether or not it is fraud\n",
    "                \n",
    "                if log_probs[i] < test_epsilon:\n",
    "                    if flag == 1: tps += 1\n",
    "                    else: fps += 1\n",
    "                else:\n",
    "                    if flag == 1: fns += 1\n",
    "                    else: tns += 1\n",
    "            \n",
    "            # if the score is the best, it becomes the best score\n",
    "            fscore = score_function(tps, fps, tns, fns)\n",
    "            if fscore > best_score:\n",
    "                epsilon = test_epsilon\n",
    "                best_score = fscore\n",
    "\n",
    "    #initialize the return values\n",
    "    fp, tp, fn, tn = 0,0,0,0\n",
    "    flags = 0\n",
    "\n",
    "    for i in range(log_probs.shape[0]):\n",
    "        #determine whether or not the element is actually fraud\n",
    "        #i + train_size gives the index in the training set. This will need to be fixed later\n",
    "        if dataset == 'ether': \n",
    "            flag = transactions_df.iloc[i + train_size][\"FLAG\"]\n",
    "        elif dataset == 'credit': \n",
    "            flag = transactions_df.iloc[i + train_size]['is_fraud']\n",
    "\n",
    "        if not flag_indices is None:\n",
    "            if dataset == 'ether': \n",
    "                flag = transactions_df.iloc[flag_indices[i]][\"FLAG\"]\n",
    "            elif dataset == 'credit': \n",
    "                flag = transactions_df.iloc[flag_indices[i]]['is_fraud']\n",
    "\n",
    "        # if the element is not fraud\n",
    "        if flag == 0:\n",
    "            #if it is marked as fraud\n",
    "            if log_probs[i] < epsilon: \n",
    "                #increment false positives\n",
    "                fp += 1\n",
    "            else: \n",
    "                #increment true negatives\n",
    "                tn += 1\n",
    "        #if the element is fraud\n",
    "        if flag == 1:\n",
    "            flags += 1\n",
    "            #if it is marked as fraud\n",
    "            if log_probs[i] < epsilon: \n",
    "                #increment true positives\n",
    "                tp += 1\n",
    "            else: \n",
    "                #increment false negatives\n",
    "                fn += 1\n",
    "\n",
    "    # keeping these for debug purposes\n",
    "    # print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "    # print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%), incorrectly classified normal: {fp}/{(X_test.shape[0]-flags)} ({fp/(X_test.shape[0]-flags)*100}%)')\n",
    "    # print(f'Normal Labels: {[i for i in sorted(zip(good_labels.keys(), good_labels.values()), key=lambda a: -a[1])]}')\n",
    "    # print(f'Fraudulent Labels: {[i for i in sorted(zip(evil_labels.keys(), evil_labels.values()), key=lambda a: -a[1])]}')\n",
    "\n",
    "    return fp, tp, fn, tn, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kept separate so that I don't accidentally fuck up my experiments by running it\n",
    "#use to reset the training\n",
    "trained_models = {}\n",
    "output_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you only need to delete this\n",
    "output_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use try catch to prevent errors if the save files are not present\n",
    "\n",
    "try:\n",
    "    #if it exists, load the previously trained models\n",
    "    model_input_file = open(model_save_file, 'rb')\n",
    "\n",
    "    #store them back in the trained models dictionary\n",
    "    unpickler = pickle.Unpickler(model_input_file)\n",
    "    trained_models = unpickler.load()\n",
    "\n",
    "    model_input_file.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    #if it exists, load the previously gathered data\n",
    "    info_input_file = open(info_save_file, 'rb')\n",
    "\n",
    "    #store them back in the output info dictionary\n",
    "    unpickler = pickle.Unpickler(info_input_file)\n",
    "    output_info = unpickler.load()\n",
    "\n",
    "    info_input_file.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = BayesianGaussianMixture().get_params(0)\n",
    "\n",
    "default['n_components'] = 10 #the defualt is 1, which is useless\n",
    "default['weight_concentration_prior'] = 10 #the default is 0, which is bad\n",
    "default['verbose'] = False #switch to true, to fill your screen with numbers\n",
    "default['max_iter'] = 1000 #defaults to 100, but that often fails to converge\n",
    "default['random_state'] = 0 #to keep experiments consistent\n",
    "\n",
    "def bgm(parameters):\n",
    "    '''Returns a BayesianGaussianMixture based on the parameters\\n\n",
    "    parameters can be either a key or a dictionary\n",
    "    '''\n",
    "    params = dict(parameters)\n",
    "    comps = default\n",
    "\n",
    "    for key in params:\n",
    "        comps[key] = params[key]\n",
    "\n",
    "    return BayesianGaussianMixture(covariance_prior=comps['covariance_prior'],\n",
    "        covariance_type=comps['covariance_type'],\n",
    "        degrees_of_freedom_prior=comps['degrees_of_freedom_prior'],\n",
    "        init_params=comps['init_params'],\n",
    "        max_iter=comps['max_iter'],\n",
    "        mean_precision_prior=comps['mean_precision_prior'],\n",
    "        mean_prior=comps['mean_prior'],\n",
    "        n_components=comps['n_components'],\n",
    "        n_init=comps['n_init'],\n",
    "        random_state=comps['random_state'],\n",
    "        reg_covar=comps['reg_covar'],\n",
    "        tol=comps['tol'],\n",
    "        verbose=comps['verbose'],\n",
    "        verbose_interval=comps['verbose_interval'],\n",
    "        warm_start=comps['warm_start'],\n",
    "        weight_concentration_prior=comps['weight_concentration_prior'],\n",
    "        weight_concentration_prior_type=comps['weight_concentration_prior_type'])\n",
    "\n",
    "def key(dic, additional_params:dict={}):\n",
    "    '''uses a parameter dictionary to save keys for easier lookup, from a dictionary\\n\n",
    "    Ex. trained_models[key({'tol':1.0, 'fold':2})] will give back the model with those parameters'''\n",
    "    dictionary = dict(dic)\n",
    "    for param in additional_params:\n",
    "        dictionary[param] = additional_params[param]\n",
    "\n",
    "    return frozenset(sorted(zip(dictionary.keys(), dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of the positions from the transactions which are fraud\n",
    "fraud_indices = df_transactions_copy[df_transactions_copy['is_fraud']==1]['Unnamed: 0'].index.to_numpy()\n",
    "true_indices = df_transactions_copy[df_transactions_copy['is_fraud']==0]['Unnamed: 0'].index.to_numpy()\n",
    "\n",
    "#remove all transactions which are not in the training set (this list is used to generate training examples)\n",
    "fraud_indices = fraud_indices[fraud_indices < X.shape[0]]\n",
    "true_indices = true_indices[true_indices < X.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trait_in_key(key_, trait):\n",
    "    #returns True if a trait is an element of a key object\n",
    "\n",
    "    key_dict = dict(key_)\n",
    "\n",
    "    return trait in key_dict\n",
    "\n",
    "def trait_from_key(key_, trait):\n",
    "    #returns the value of the specified trait in a key object, or None if it's not present\n",
    "    \n",
    "    key_dict = dict(key_)\n",
    "\n",
    "    if trait in key_dict:\n",
    "        return key_dict[trait]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the models to be trained, as a list of keys\n",
    "test_models = [key({}) for r in range(1)]\n",
    "\n",
    "#makes the training set the first train_size elements and the test set the next test_size\n",
    "train_size = 50000\n",
    "test_size = 1000\n",
    "# X_train = X[np.array(random.choices(true_indices, k=train_size))]\n",
    "train_indices = true_indices[:train_size]\n",
    "X_train = X[train_indices]\n",
    "\n",
    "#add a few extra fraud examples to the test set, for balance\n",
    "test_indices = np.concatenate((np.array(random.choices(true_indices, k=test_size), dtype=int),np.array(random.choices(fraud_indices, k=500), dtype=int)))\n",
    "X_test = X[test_indices]\n",
    "\n",
    "#for each key\n",
    "for model in test_models:\n",
    "    #if the key is not already in the trained dictionary\n",
    "    if not model in trained_models:\n",
    "        # if lastr != trait_from_key(model, 'run'):\n",
    "        #     X_train = X[np.array(random.choices(true_indices, k=train_size))]\n",
    "\n",
    "        #generate a model with parameters that match the key\n",
    "        m = bgm(model)\n",
    "        \n",
    "        #fit the model to the training data\n",
    "        m.fit(X_train)\n",
    "\n",
    "        #print the model so that you don't have to worry about whether or not the code is working\n",
    "        print(model)\n",
    "\n",
    "        #save the model to the trained models\n",
    "        trained_models[model] = m\n",
    "\n",
    "for model in trained_models:\n",
    "    # if the key is not already in the data dictionary\n",
    "    if not model in output_info:\n",
    "        #generate the output info, as a labeled dictionary\n",
    "        info = {}\n",
    "        info['labels'] = trained_models[model].predict(X_test)\n",
    "        info['probs'] = trained_models[model].predict_proba(X_test)\n",
    "        info['log_probs'] = trained_models[model].score_samples(X_test)\n",
    "        info['test_log_probs'] = trained_models[model].score_samples(X_test_final)\n",
    "        info['scores'] = get_measurements(log_probs=info['log_probs'], \n",
    "                                          transactions_df=df_transactions_copy, \n",
    "                                          train_size=train_size, \n",
    "                                          score_function=lambda tps,fps,tns,fns: tps - fps + tns - fns,\n",
    "                                          flag_indices=test_indices.astype(int), \n",
    "                                          inverted=False)\n",
    "        info['test_scores'] = get_measurements(log_probs=info['test_log_probs'], \n",
    "                                               transactions_df=df_transactions_copy, \n",
    "                                               flag_indices=final_test_indices, \n",
    "                                               inverted=False,\n",
    "                                               dynamic_epsilon=False, \n",
    "                                               epsilon=info['scores'][4])\n",
    "\n",
    "        #set this last, so that data isn't saved with only half the work done\n",
    "        output_info[model] = info\n",
    "        print(f'generated output for {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: this overwrites the previous save files \n",
    "# (this is negated by making sure that the earlier block which adds the save file back into memory\n",
    "# was run)\n",
    "\n",
    "#save the trained models\n",
    "model_output_file = open(model_save_file, 'wb')\n",
    "models_pickler = pickle.Pickler(model_output_file)\n",
    "models_pickler.dump(trained_models)\n",
    "model_output_file.close()\n",
    "\n",
    "#save the test info\n",
    "info_output_file = open(info_save_file, 'wb')\n",
    "info_pickler = pickle.Pickler(info_output_file)\n",
    "info_pickler.dump(output_info)\n",
    "info_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the stats, for the benefit of everyone who is not a computer\n",
    "for model in output_info:\n",
    "\n",
    "    fp, tp, fn, tn, epsilon = output_info[model]['scores']\n",
    "    #number of transactions marked as suspicious\n",
    "    pos = fp + tp\n",
    "    #number of transactions ignored\n",
    "    neg = fn + tn\n",
    "    #number of fraudulent transactions\n",
    "    flags = tp + fn\n",
    "\n",
    "    #human readible output\n",
    "    print(dict(model))\n",
    "    print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "    print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%)')\n",
    "    print(f'incorrectly classified normal: {fp}/{(output_info[model]['log_probs'].shape[0]-flags)} ({fp/(output_info[model]['log_probs'].shape[0]-flags)*100}%)')\n",
    "\n",
    "    #check to make sure the output was generated with the final test set included\n",
    "    if 'test_scores' in output_info[model]:\n",
    "        fp, tp, fn, tn, epsilon = output_info[model]['test_scores']\n",
    "        #number of transactions marked as suspicious\n",
    "        pos = fp + tp\n",
    "        #number of transactions ignored\n",
    "        neg = fn + tn\n",
    "        #number of fraudulent transactions\n",
    "        flags = tp + fn\n",
    "\n",
    "        #human readible output\n",
    "        print(f'*Test Set*')\n",
    "        print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "        print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%)')\n",
    "        print(f'incorrectly classified normal: {fp}/{(output_info[model]['test_log_probs'].shape[0]-flags)} ({fp/(output_info[model]['test_log_probs'].shape[0]-flags)*100}%)')\n",
    "        print()\n",
    "    else:\n",
    "        print(f'please delete {info_save_file} and rerun the generator; the output file is incomplete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
