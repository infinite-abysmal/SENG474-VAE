{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn import model_selection\n",
    "\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to either ibm or ether; NOTE: ibm not currently working for a few later functions\n",
    "dataset = 'ether'\n",
    "\n",
    "#NOTE: I added these to the gitignore\n",
    "model_save_file = 'stored_models.pickle'\n",
    "info_save_file = 'model_info.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'ibm':\n",
    "    df_transactions = pd.read_csv(\"archive/User0_credit_card_transactions.csv\")\n",
    "    df_cards = pd.read_csv(\"archive/sd254_cards.csv\")\n",
    "    df_users = pd.read_csv(\"archive/sd254_users.csv\")\n",
    "\n",
    "    #create a copy in which the fraud flag is enabled, for later reference\n",
    "    df_transactions_copy = df_transactions.copy()\n",
    "\n",
    "    # get rid of the fraud column to prevent really stupid and embarrasing outcomes\n",
    "    df_transactions = df_transactions.drop(columns=[\"Is Fraud?\"])\n",
    "\n",
    "    #these are used when converting to sample vectors\n",
    "    transactions = df_transactions.to_numpy()\n",
    "    cards = df_cards.to_numpy()\n",
    "    users = df_users.to_numpy()\n",
    "\n",
    "if dataset == 'ether':\n",
    "    df_transactions = pd.read_csv('archive/transaction_dataset.csv')\n",
    "\n",
    "    #create a copy in which the fraud flag is enabled, for later reference\n",
    "    df_transactions_copy = df_transactions.copy()\n",
    "\n",
    "    #these rows are all useless/would cause bad outcomes\n",
    "    df_transactions = df_transactions.drop(columns=['Unnamed: 0', 'FLAG', 'Index'])\n",
    "\n",
    "    transactions = df_transactions.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: IGNORE THIS BLOCK, I AMO GOING TO REWRITE IT SOON\n",
    "if dataset == \"ibm\":\n",
    "\n",
    "    #need to convert the CSV text values into numbers\n",
    "\n",
    "    #transaction data\n",
    "    num_transaction_types = 0\n",
    "    num_merchant_cities = 0\n",
    "    num_merchant_states = 0\n",
    "    num_errors = 0\n",
    "\n",
    "    #user data\n",
    "    num_genders = 0\n",
    "    num_user_cities = 0\n",
    "    num_user_states = 0\n",
    "\n",
    "    #card data\n",
    "    num_brands = 0\n",
    "    num_card_types = 0\n",
    "\n",
    "    #transaction data\n",
    "    transaction_types = {}\n",
    "    merchant_cities = {}\n",
    "    merchant_states = {}\n",
    "    errors = {}\n",
    "\n",
    "    #user data\n",
    "    genders = {}\n",
    "    user_cities = {}\n",
    "    user_states = {}\n",
    "\n",
    "    #card data\n",
    "    card_brands = {}\n",
    "    card_types = {}\n",
    "\n",
    "    zero_time = datetime.datetime.strptime(\"00:00\", \"%H:%M\")\n",
    "\n",
    "    X = np.ndarray((transactions.shape[0], transactions.shape[1] + df_cards.shape[1] + df_users.shape[1]), dtype=float)\n",
    "    X_reference = np.ndarray(X.shape, dtype=object)\n",
    "\n",
    "    for i in range(transactions.shape[0]):\n",
    "        user = transactions[i, 0]\n",
    "        card = transactions[i, 1]\n",
    "\n",
    "        card_data = df_cards[(df_cards['User'] == user) & (df_cards['CARD INDEX'] == card)]\n",
    "\n",
    "        #these values are already numbers; no need to change them\n",
    "        #TODO: normalize some of them?\n",
    "        X[i, 0:5] = transactions[i, 0:5]\n",
    "        X[i, 8] = transactions[i, 8]\n",
    "        X[i, 11:13] = transactions[i, 11:13]\n",
    "\n",
    "        X_reference[i, :transactions.shape[1]] = transactions[i, :]\n",
    "\n",
    "        #convert the time of date string into the number of minutes past midnight\n",
    "        dtime = datetime.datetime.strptime(transactions[i, 5], \"%H:%M\")\n",
    "        X[i, 5] = (dtime - zero_time).seconds//60\n",
    "\n",
    "        #convert dollar amount from \"$134.03\" to 134.03\n",
    "        X[i, 6] = float(transactions[i, 6][1:])\n",
    "\n",
    "        #use ints to represent the different transaction types\n",
    "        if not transactions[i, 7] in transaction_types:\n",
    "            transaction_types[transactions[i, 7]] = num_transaction_types\n",
    "            num_transaction_types += 1\n",
    "        X[i, 7] = transaction_types[transactions[i, 7]]\n",
    "\n",
    "        #use ints to represent the different cities\n",
    "        if not transactions[i, 9] in merchant_cities:\n",
    "            merchant_cities[transactions[i, 9]] = num_merchant_cities\n",
    "            num_merchant_cities += 1\n",
    "        X[i, 9] = merchant_cities[transactions[i, 9]]\n",
    "\n",
    "        #use ints to represent the different states\n",
    "        if not transactions[i, 10] in merchant_states:\n",
    "            merchant_states[transactions[i, 10]] = num_merchant_states\n",
    "            num_merchant_states += 1\n",
    "        X[i, 10] = merchant_states[transactions[i, 10]]\n",
    "\n",
    "        #use ints to represent errors\n",
    "        if not transactions[i, 13] in errors:\n",
    "            errors[transactions[i, 13]] = num_errors\n",
    "            num_errors += 1\n",
    "        X[i, 13] = 0 #errors[transactions[i, 13]]\n",
    "        #I disabled this because it caused too many outliers\n",
    "\n",
    "        X[i, 14:18] = users[user, 1:5] \n",
    "        X_reference[i, transactions.shape[1]:transactions.shape[1] + users.shape[1] - 1] = users[user, 1:]\n",
    "\n",
    "        #use ints to represent gender \n",
    "        #(pretty sure the csv only has male/female, but don't want to cause any issues)\n",
    "        if not users[user, 5] in genders:\n",
    "            genders[users[user, 5]] = num_genders\n",
    "            num_genders += 1\n",
    "        X[i, 18] = genders[users[user, 5]]\n",
    "\n",
    "        #evey address is unique, I'm just going to leave it blank\n",
    "        X[i, 19] = 0\n",
    "\n",
    "        #appartment; probably not relevant\n",
    "        X[i, 20] = users[user, 7]\n",
    "\n",
    "        #use ints to represent cities\n",
    "        if not users[user, 8] in user_cities:\n",
    "            user_cities[users[user, 8]] = num_user_cities\n",
    "            num_user_cities += 1\n",
    "        X[i, 21] = user_cities[users[user, 8]]\n",
    "\n",
    "        #use ints to represent states\n",
    "        if not users[user, 9] in user_states:\n",
    "            user_states[users[user, 9]] = num_user_states\n",
    "            num_user_states += 1\n",
    "        X[i, 22] = user_states[users[user, 9]]\n",
    "\n",
    "        #ZIP, lat, long\n",
    "        X[i, 23:26] = users[user, 10:13]\n",
    "\n",
    "        #per capita - ZIP\n",
    "        X[i, 26] = float(users[user, 13][1:])\n",
    "\n",
    "        #yearly income\n",
    "        X[i, 27] = float(users[user, 14][1:])\n",
    "\n",
    "        #debt\n",
    "        X[i, 28] = float(users[user, 15][1:])\n",
    "\n",
    "        #FICO, Num credit cards\n",
    "        X[i, 29:31] = users[user, 16:19]\n",
    "\n",
    "        card_data_np = card_data.to_numpy()\n",
    "        X_reference[i, transactions.shape[1] + users.shape[1] - 1:transactions.shape[1] + users.shape[1] + card_data_np.shape[1] - 3] = card_data_np[0][2:]\n",
    "        #turns out I could have been using dataframes, rather than np\n",
    "        if i == 5011: print(i, user, card_data['Card Brand'])\n",
    "        if not card_data['Card Brand'][card] in card_brands:\n",
    "            card_brands[card_data['Card Brand'][card]] = num_brands\n",
    "            num_brands += 1\n",
    "        X[i, 31] = card_brands[card_data['Card Brand'][card]]\n",
    "\n",
    "        #use ints to represent card types\n",
    "        if not card_data['Card Type'][card] in card_types:\n",
    "            card_types[card_data['Card Type'][card]] = num_card_types\n",
    "            num_card_types += 1\n",
    "        X[i, 32] = card_types[card_data['Card Type'][card]]\n",
    "\n",
    "        X[i, 33] = card_data['Card Number'][card]\n",
    "\n",
    "        #read expiry date as months\n",
    "        X[i, 34] = float(card_data['Expires'][card][0:2]) + float(card_data['Expires'][card][3:8]) * 12\n",
    "        \n",
    "        X[i, 35] = card_data['CVV'][card]\n",
    "\n",
    "        X[i, 36] = 0 if card_data['Has Chip'][card] == 'No' else 1\n",
    "\n",
    "        X[i, 37] = card_data['Cards Issued'][card]\n",
    "\n",
    "        X[i, 38] = float(card_data[\"Credit Limit\"][card][1:])\n",
    "\n",
    "        X[i, 39] = float(card_data['Acct Open Date'][card][0:2]) + float(card_data['Acct Open Date'][card][3:8]) * 12\n",
    "\n",
    "        X[i, 40] = card_data['Year PIN last Changed'][card]\n",
    "\n",
    "        X[i, 41] = 0 if card_data['Card on Dark Web'][card] == 'No' else 1\n",
    "\n",
    "        #generate extra information section\n",
    "\n",
    "        #time since last transaction (0 for first transatciont)\n",
    "        if i > 0 and df_transactions['User'][i] == df_transactions['User'][i-1]:\n",
    "            transaction_time = datetime.datetime(df_transactions[\"Year\"][i], df_transactions[\"Month\"][i], df_transactions[\"Day\"][i], int(df_transactions[\"Time\"][i][0:2]), int(df_transactions['Time'][i][3:]))\n",
    "            last_transaction_time = datetime.datetime(df_transactions[\"Year\"][i - 1], df_transactions[\"Month\"][i - 1], df_transactions[\"Day\"][i - 1], int(df_transactions[\"Time\"][i - 1][0:2]), int(df_transactions['Time'][i - 1][3:]))\n",
    "            relative_time = transaction_time - last_transaction_time\n",
    "            # print(relative_time)\n",
    "            X[i, 42] = relative_time.total_seconds()\n",
    "        X_reference[i, 42] = float(X[i, 42])\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            if math.isnan(X[i, j]): X[i, j] = 0\n",
    "\n",
    "        \n",
    "\n",
    "        # print(transactions[i])    \n",
    "        # print(X[i], X[i, 11], transactions[i, 11])\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i])\n",
    "        print(X_reference[i])\n",
    "    # print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'ether':\n",
    "\n",
    "    unique_indices = {}\n",
    "    unique_indices_values = {}\n",
    "\n",
    "    X = np.ndarray(transactions.shape)\n",
    "\n",
    "    #this section is kind of abstract\n",
    "    #for every element of the dataset\n",
    "    for i in range(X.shape[0]):\n",
    "        #for every component of that element\n",
    "        for j in range(X.shape[1]):\n",
    "            #if that component is a number, it stays as it is\n",
    "            if isinstance(transactions[i, j], int) or isinstance(transactions[i, j], float):\n",
    "                X[i, j] = transactions[i, j]\n",
    "            \n",
    "            #if it is not a number, assign an integer to each unique value which appears\n",
    "            #ex, if name was a column, we might have 'Bob'=1, 'Cindy'=2, etc\n",
    "            else:\n",
    "                if not j in unique_indices:\n",
    "                    unique_indices[j] = {}\n",
    "                    unique_indices_values[j] = 0\n",
    "                if not transactions[i, j] in unique_indices[j]:\n",
    "                    unique_indices[j][transactions[i, j]] = unique_indices_values[j]\n",
    "                    unique_indices_values[j] += 1\n",
    "                X[i, j] = unique_indices[j][transactions[i, j]]\n",
    "            \n",
    "            #replace nan values with 0, since nan is not permissable in a BGM\n",
    "            #might be wise to replace with something else?\n",
    "            if math.isnan(X[i, j]):\n",
    "                X[i, j] = 0\n",
    "    \n",
    "    #in the ether dataset, index 22 breaks the solver, for some reason; remove it\n",
    "    excluded_indices = [22]\n",
    "    included_indices = [i for i in range(X.shape[1]) if i not in excluded_indices]\n",
    "\n",
    "    #I was pleasantly surprised numpy allowed this type of indexing\n",
    "    X = X[:, included_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements(log_probs:np.ndarray, transactions_df:pd.DataFrame, train_size:int=0, \n",
    "                     dynamic_epsilon:bool=True, \n",
    "                     score_function:None=lambda tps,fps,tns,fns: tps**1.3 - fps + tns - fns**1.3, \n",
    "                     epsilon:float=-200, test_range:int=400, test_step:int=5):\n",
    "    '''\n",
    "    gets the number of False Positives, True Positives, False Negatives, \n",
    "    True Negatives, and the best epsilon (if dynamic_epsilon=True)\n",
    "    '''\n",
    "\n",
    "    # determine which epsilon gives the best results (if dynamic is on)\n",
    "    best_score = 0\n",
    "    if dynamic_epsilon:\n",
    "        # for each test epsilon in range \n",
    "        # Max epsilon -> Max epsilon - test range (to prevent cases with insane length)\n",
    "        for test_epsilon in range(int(max(log_probs)), int(max(log_probs))-test_range, -test_step):\n",
    "            tps = 0\n",
    "            fps = 0\n",
    "            tns = 0\n",
    "            fns = 0\n",
    "\n",
    "            # for each probability\n",
    "            for i in range(log_probs.shape[0]):\n",
    "                #get whether or not a data point is fraud; NOTE: needs a rework\n",
    "                flag = transactions_df.iloc[i + train_size][\"FLAG\"]\n",
    "                \n",
    "                # if the element is marked as fraud, determine whether or not it is fraud\n",
    "                if log_probs[i] < test_epsilon:\n",
    "                    if flag == 1: tps += 1\n",
    "                    else: fps += 1\n",
    "                else:\n",
    "                   if flag == 1: fns += 1\n",
    "                   else: tns += 1 \n",
    "            \n",
    "            # if the score is the best, it becomes the best score\n",
    "            fscore = score_function(tps, fps, tns, fns)\n",
    "            if fscore > best_score:\n",
    "                epsilon = test_epsilon\n",
    "                best_score = fscore\n",
    "\n",
    "    #initialize the return values\n",
    "    fp, tp, fn, tn = 0,0,0,0\n",
    "    flags = 0\n",
    "\n",
    "    for i in range(log_probs.shape[0]):\n",
    "        #determine whether or not the element is actually fraud\n",
    "        #i + train_size gives the index in the training set. This will need to be fixed later\n",
    "        flag = transactions_df.iloc[i + train_size][\"FLAG\"]\n",
    "\n",
    "        # if the element is not fraud\n",
    "        if flag == 0:\n",
    "            #if it is marked as fraud\n",
    "            if log_probs[i] < epsilon: \n",
    "                #increment false positives\n",
    "                fp += 1\n",
    "            else: \n",
    "                #increment true negatives\n",
    "                tn += 1\n",
    "        #if the element is fraud\n",
    "        if flag == 1:\n",
    "            flags += 1\n",
    "            #if it is marked as fraud\n",
    "            if log_probs[i] < epsilon: \n",
    "                #increment true positives\n",
    "                tp += 1\n",
    "            else: \n",
    "                #increment false negatives\n",
    "                fn += 1\n",
    "\n",
    "    # keeping these for debug purposes\n",
    "    # print(f'FP:{fp}, TP:{tp}, FN:{fn}, TN:{tn}, pos:{pos}, neg:{neg}, flags:{flags}')\n",
    "    # print(f'correctly classified fraud:{tp}/{flags} ({tp/flags*100}%), incorrectly classified normal: {fp}/{(X_test.shape[0]-flags)} ({fp/(X_test.shape[0]-flags)*100}%)')\n",
    "    # print(f'Normal Labels: {[i for i in sorted(zip(good_labels.keys(), good_labels.values()), key=lambda a: -a[1])]}')\n",
    "    # print(f'Fraudulent Labels: {[i for i in sorted(zip(evil_labels.keys(), evil_labels.values()), key=lambda a: -a[1])]}')\n",
    "\n",
    "    return fp, tp, fn, tn, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kept separate so that I don't accidentally fuck up my experiments by running it\n",
    "#use to reset the training\n",
    "trained_models = {}\n",
    "output_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use try catch to prevent errors if the save files are not present\n",
    "\n",
    "try:\n",
    "    #if it exists, load the previously trained models\n",
    "    model_input_file = open(model_save_file, 'rb')\n",
    "\n",
    "    #store them back in the trained models dictionary\n",
    "    unpickler = pickle.Unpickler(model_input_file)\n",
    "    trained_models = unpickler.load()\n",
    "\n",
    "    model_input_file.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    #if it exists, load the previously gathered data\n",
    "    info_input_file = open(info_save_file, 'rb')\n",
    "\n",
    "    #store them back in the output info dictionary\n",
    "    unpickler = pickle.Unpickler(info_input_file)\n",
    "    output_info = unpickler.load()\n",
    "\n",
    "    info_input_file.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = BayesianGaussianMixture().get_params(0)\n",
    "\n",
    "default['n_components'] = 10 #the defualt is 1, which is useless\n",
    "default['weight_concentration_prior'] = 10 #the default is 0, which is bad\n",
    "default['verbose'] = False #switch to true, to fill your screen with numbers\n",
    "default['max_iter'] = 1000 #defaults to 100, but that often fails to converge\n",
    "default['random_state'] = 0 #to keep experiments consistent\n",
    "\n",
    "def bgm(parameters):\n",
    "    '''Returns a BayesianGaussianMixture based on the parameters\\n\n",
    "    parameters can be either a key or a dictionary\n",
    "    '''\n",
    "    params = dict(parameters)\n",
    "    comps = default\n",
    "\n",
    "    for key in params:\n",
    "        comps[key] = params[key]\n",
    "\n",
    "    return BayesianGaussianMixture(covariance_prior=comps['covariance_prior'],\n",
    "        covariance_type=comps['covariance_type'],\n",
    "        degrees_of_freedom_prior=comps['degrees_of_freedom_prior'],\n",
    "        init_params=comps['init_params'],\n",
    "        max_iter=comps['max_iter'],\n",
    "        mean_precision_prior=comps['mean_precision_prior'],\n",
    "        mean_prior=comps['mean_prior'],\n",
    "        n_components=comps['n_components'],\n",
    "        n_init=comps['n_init'],\n",
    "        random_state=comps['random_state'],\n",
    "        reg_covar=comps['reg_covar'],\n",
    "        tol=comps['tol'],\n",
    "        verbose=comps['verbose'],\n",
    "        verbose_interval=comps['verbose_interval'],\n",
    "        warm_start=comps['warm_start'],\n",
    "        weight_concentration_prior=comps['weight_concentration_prior'],\n",
    "        weight_concentration_prior_type=comps['weight_concentration_prior_type'])\n",
    "\n",
    "def key(dic, additional_params:dict={}):\n",
    "    '''uses a parameter dictionary to save keys for easier lookup, from a dictionary\\n\n",
    "    Ex. trained_models[key({'tol':1.0, 'fold':2})] will give back the model with those parameters'''\n",
    "    dictionary = dict(dic)\n",
    "    for param in additional_params:\n",
    "        dictionary[param] = additional_params[param]\n",
    "\n",
    "    return frozenset(sorted(zip(dictionary.keys(), dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the models to be trained, as a list of keys\n",
    "test_models = [key({'weight_concentration_prior':float(i)}) for i in range(10, 20, 5)]\n",
    "\n",
    "#makes the training set the first 5000 elements and the test set the last 4800\n",
    "train_size = 5000\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "\n",
    "#for each key\n",
    "for model in test_models:\n",
    "    #if the key is not already in the trained dictionary\n",
    "    if not model in trained_models:\n",
    "        #generate a model with parameters that match the key\n",
    "        m = bgm(model)\n",
    "        \n",
    "        #fit the model to the training data\n",
    "        m.fit(X_train)\n",
    "\n",
    "        #print the model so that you don't have to worry about whether or not the code is working\n",
    "        print(model)\n",
    "\n",
    "        #save the model to the trained models\n",
    "        trained_models[model] = m\n",
    "    \n",
    "    # if the key is not already in the data dictionary\n",
    "    if not model in output_info:\n",
    "        #generate the output info, as a labeled dictionary\n",
    "        output_info[model] = {}\n",
    "        output_info[model]['labels'] = trained_models[model].predict(X_test)\n",
    "        output_info[model]['probs'] = trained_models[model].predict_proba(X_test)\n",
    "        output_info[model]['log_probs'] = trained_models[model].score_samples(X_test)\n",
    "        output_info[model]['scores'] = get_measurements(output_info[model]['log_probs'], df_transactions_copy, train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: this overwrites the previous save files \n",
    "# (this is negated by making sure that the earlier block which adds the save file back into memory\n",
    "# was run)\n",
    "\n",
    "#save the trained models\n",
    "model_output_file = open(model_save_file, 'wb')\n",
    "models_pickler = pickle.Pickler(model_output_file)\n",
    "models_pickler.dump(trained_models)\n",
    "model_output_file.close()\n",
    "\n",
    "#save the test info\n",
    "info_output_file = open(info_save_file, 'wb')\n",
    "info_pickler = pickle.Pickler(info_output_file)\n",
    "info_pickler.dump(output_info)\n",
    "info_output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
